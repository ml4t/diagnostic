{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML4T Diagnostic: End-to-End Trade Diagnostics Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow for diagnosing ML trading strategy errors using **ml4t-diagnostic**.\n",
    "\n",
    "## What We'll Cover\n",
    "\n",
    "1. **Generate Synthetic Backtest Data** - Create realistic trading data with intentional error patterns\n",
    "2. **Basic Trade Analysis** - Identify worst/best trades and compute statistics\n",
    "3. **Statistical Validation (DSR)** - Validate strategy significance with Deflated Sharpe Ratio\n",
    "4. **SHAP Analysis** - Explain worst trades using SHAP values\n",
    "5. **Error Pattern Clustering** - Discover recurring failure modes\n",
    "6. **Hypothesis Generation** - Get actionable improvement suggestions\n",
    "7. **Dashboard Exploration** - Interactive visualization (Streamlit)\n",
    "8. **Summary & Next Steps** - Implementation roadmap\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install ml4t-diagnostic with ML dependencies\n",
    "pip install ml4t-diagnostic[ml]\n",
    "\n",
    "# For dashboard (optional)\n",
    "pip install ml4t-diagnostic[dashboard]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Add src to path for local development (if needed)\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ML4T Diagnostic\n",
    "from ml4t.diagnostic.integration.backtest_contract import TradeRecord\n",
    "from ml4t.diagnostic.evaluation import (\n",
    "    TradeAnalysis,\n",
    "    TradeShapAnalyzer,\n",
    ")\n",
    "from ml4t.diagnostic.evaluation.stats import deflated_sharpe_ratio\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\u2705 All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Backtest Data\n",
    "\n",
    "We'll create a realistic cryptocurrency futures backtest with **75 trades** over 6 months. The data includes:\n",
    "\n",
    "- **Three distinct error patterns**:\n",
    "  1. **High volatility + Momentum failures** (25 trades) - Entering low-vol momentum that reverses\n",
    "  2. **Low liquidity + Trend reversals** (25 trades) - Poor execution in illiquid markets\n",
    "  3. **Regime changes + Correlation breaks** (25 trades) - Market regime shifts\n",
    "\n",
    "- **10 features** with realistic values\n",
    "- **SHAP values** that create distinct clusters\n",
    "- **40% win rate** (realistic for many strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_backtest(n_trades=75, seed=42):",
    "    \"\"\"Generate realistic synthetic backtest with intentional error patterns.\"\"\"",
    "    np.random.seed(seed)",
    "    ",
    "    # Feature names",
    "    feature_names = [",
    "        'momentum_5d', 'volatility_20d', 'rsi_14', 'volume_ratio',",
    "        'trend_strength', 'liquidity', 'correlation', 'skewness',",
    "        'kurtosis', 'regime_prob'",
    "    ]",
    "    ",
    "    # Symbols",
    "    symbols = ['BTC-PERP', 'ETH-PERP', 'SOL-PERP', 'MATIC-PERP']",
    "    ",
    "    # Start date",
    "    start_date = datetime(2024, 1, 1)",
    "    ",
    "    # Storage",
    "    trades = []",
    "    feature_matrix = []",
    "    shap_matrix = []",
    "    ",
    "    # Generate 3 patterns (25 trades each)",
    "    trades_per_pattern = n_trades // 3",
    "    ",
    "    for pattern_id in range(3):",
    "        for i in range(trades_per_pattern):",
    "            # Generate timestamp (spread over 6 months)",
    "            days_offset = np.random.randint(0, 180)",
    "            hours_offset = np.random.randint(0, 24)",
    "            timestamp = start_date + timedelta(days=days_offset, hours=hours_offset)",
    "            ",
    "            # Pattern-specific feature generation",
    "            if pattern_id == 0:",
    "                # Pattern 1: High momentum + Low volatility \u2192 Losses",
    "                features = {",
    "                    'momentum_5d': np.random.uniform(1.5, 3.0),      # High momentum",
    "                    'volatility_20d': np.random.uniform(0.001, 0.01), # Low volatility",
    "                    'rsi_14': np.random.uniform(60, 80),             # Overbought",
    "                    'volume_ratio': np.random.uniform(0.8, 1.5),",
    "                    'trend_strength': np.random.uniform(0.6, 0.9),",
    "                    'liquidity': np.random.uniform(0.5, 1.0),",
    "                    'correlation': np.random.uniform(0.3, 0.7),",
    "                    'skewness': np.random.uniform(-0.5, 0.5),",
    "                    'kurtosis': np.random.uniform(2.5, 4.0),",
    "                    'regime_prob': np.random.uniform(0.4, 0.7)",
    "                }",
    "                # SHAP values highlight momentum and volatility",
    "                shap_values = {",
    "                    'momentum_5d': np.random.uniform(0.35, 0.55),    # Positive contribution to loss",
    "                    'volatility_20d': np.random.uniform(-0.40, -0.25), # Negative contribution",
    "                    'rsi_14': np.random.uniform(0.20, 0.35),",
    "                    'volume_ratio': np.random.uniform(-0.1, 0.1),",
    "                    'trend_strength': np.random.uniform(-0.1, 0.1),",
    "                    'liquidity': np.random.uniform(-0.1, 0.1),",
    "                    'correlation': np.random.uniform(-0.1, 0.1),",
    "                    'skewness': np.random.uniform(-0.1, 0.1),",
    "                    'kurtosis': np.random.uniform(-0.1, 0.1),",
    "                    'regime_prob': np.random.uniform(-0.1, 0.1)",
    "                }",
    "                # Mostly losses (80%)",
    "                is_loss = np.random.random() < 0.8",
    "                ",
    "            elif pattern_id == 1:",
    "                # Pattern 2: Low liquidity + Wide spread \u2192 Losses",
    "                features = {",
    "                    'momentum_5d': np.random.uniform(-0.5, 1.0),",
    "                    'volatility_20d': np.random.uniform(0.01, 0.03),",
    "                    'rsi_14': np.random.uniform(40, 60),",
    "                    'volume_ratio': np.random.uniform(0.3, 0.8),     # Low volume",
    "                    'trend_strength': np.random.uniform(0.3, 0.6),",
    "                    'liquidity': np.random.uniform(0.1, 0.4),        # Low liquidity",
    "                    'correlation': np.random.uniform(-0.3, 0.3),",
    "                    'skewness': np.random.uniform(-1.0, 1.0),",
    "                    'kurtosis': np.random.uniform(3.0, 6.0),",
    "                    'regime_prob': np.random.uniform(0.3, 0.6)",
    "                }",
    "                shap_values = {",
    "                    'momentum_5d': np.random.uniform(-0.1, 0.1),",
    "                    'volatility_20d': np.random.uniform(-0.1, 0.1),",
    "                    'rsi_14': np.random.uniform(-0.1, 0.1),",
    "                    'volume_ratio': np.random.uniform(0.25, 0.40),   # Positive contribution to loss",
    "                    'trend_strength': np.random.uniform(-0.1, 0.1),",
    "                    'liquidity': np.random.uniform(-0.60, -0.40),    # Negative contribution",
    "                    'correlation': np.random.uniform(-0.1, 0.1),",
    "                    'skewness': np.random.uniform(-0.1, 0.1),",
    "                    'kurtosis': np.random.uniform(-0.1, 0.1),",
    "                    'regime_prob': np.random.uniform(-0.1, 0.1)",
    "                }",
    "                is_loss = np.random.random() < 0.75",
    "                ",
    "            else:",
    "                # Pattern 3: Regime change + Correlation break \u2192 Losses",
    "                features = {",
    "                    'momentum_5d': np.random.uniform(-1.0, 1.0),",
    "                    'volatility_20d': np.random.uniform(0.02, 0.05), # High volatility",
    "                    'rsi_14': np.random.uniform(30, 70),",
    "                    'volume_ratio': np.random.uniform(0.8, 2.0),",
    "                    'trend_strength': np.random.uniform(0.2, 0.5),   # Weak trend",
    "                    'liquidity': np.random.uniform(0.5, 1.0),",
    "                    'correlation': np.random.uniform(-0.5, 0.0),     # Low/negative correlation",
    "                    'skewness': np.random.uniform(-1.5, 1.5),",
    "                    'kurtosis': np.random.uniform(4.0, 8.0),         # Fat tails",
    "                    'regime_prob': np.random.uniform(0.1, 0.3)       # Low regime confidence",
    "                }",
    "                shap_values = {",
    "                    'momentum_5d': np.random.uniform(-0.1, 0.1),",
    "                    'volatility_20d': np.random.uniform(0.30, 0.45), # Positive contribution",
    "                    'rsi_14': np.random.uniform(-0.1, 0.1),",
    "                    'volume_ratio': np.random.uniform(-0.1, 0.1),",
    "                    'trend_strength': np.random.uniform(-0.1, 0.1),",
    "                    'liquidity': np.random.uniform(-0.1, 0.1),",
    "                    'correlation': np.random.uniform(-0.40, -0.25),  # Negative contribution",
    "                    'skewness': np.random.uniform(-0.1, 0.1),",
    "                    'kurtosis': np.random.uniform(0.20, 0.35),       # Positive contribution",
    "                    'regime_prob': np.random.uniform(-0.35, -0.20)   # Negative contribution",
    "                }",
    "                is_loss = np.random.random() < 0.75",
    "            ",
    "            # Generate trade metrics",
    "            symbol = np.random.choice(symbols)",
    "            entry_price = np.random.uniform(10000, 50000)",
    "            quantity = np.random.uniform(0.1, 2.0)  # Random position size",
    "            ",
    "            # Generate return based on loss probability",
    "            if is_loss:",
    "                return_pct = np.random.uniform(-5.0, -0.5)",
    "            else:",
    "                return_pct = np.random.uniform(0.5, 4.0)",
    "            ",
    "            # Calculate exit price and PnL consistently",
    "            exit_price = entry_price * (1 + return_pct / 100)",
    "            duration = timedelta(days=np.random.uniform(0.5, 10.0))",
    "            direction = np.random.choice(['long', 'short'])",
    "            ",
    "            # Calculate PnL based on direction (must match TradeRecord validation)",
    "            if direction == \"long\":",
    "                pnl = (exit_price - entry_price) * quantity",
    "            else:",
    "                pnl = (entry_price - exit_price) * quantity",
    "            ",
    "            # Create TradeRecord",
    "            trade = TradeRecord(",
    "                timestamp=timestamp,",
    "                symbol=symbol,",
    "                entry_price=entry_price,",
    "                exit_price=exit_price,",
    "                pnl=pnl,",
    "                duration=duration,",
    "                direction=direction,",
    "                quantity=quantity",
    "            )",
    "            ",
    "            trades.append(trade)",
    "            feature_matrix.append([features[f] for f in feature_names])",
    "            shap_matrix.append([shap_values[f] for f in feature_names])",
    "    ",
    "    # Convert to arrays",
    "    features_array = np.array(feature_matrix)",
    "    shap_array = np.array(shap_matrix)",
    "    ",
    "    # Create features DataFrame with timestamps",
    "    features_df = pl.DataFrame(",
    "        {**{'timestamp': [t.timestamp for t in trades]},",
    "         **{name: features_array[:, i] for i, name in enumerate(feature_names)}}",
    "    )",
    "    ",
    "    return trades, features_df, shap_array, feature_names",
    "",
    "# Generate data",
    "print(\"Generating synthetic backtest data...\")",
    "trades, features_df, shap_values, feature_names = generate_synthetic_backtest()",
    "",
    "print(f\"\u2705 Generated {len(trades)} trades\")",
    "print(f\"\u2705 Features shape: {features_df.shape}\")",
    "print(f\"\u2705 SHAP values shape: {shap_values.shape}\")",
    "print(f\"\\nFeatures: {feature_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Trade Analysis\n",
    "\n",
    "First, let's analyze the trade distribution and identify the worst/best performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TradeAnalysis instance",
    "analyzer = TradeAnalysis(trades)",
    "",
    "# Get worst and best trades",
    "worst_trades = analyzer.worst_trades(n=20)",
    "best_trades = analyzer.best_trades(n=10)",
    "",
    "# Compute statistics",
    "stats = analyzer.compute_statistics()",
    "",
    "# Calculate Sharpe Ratio (not in TradeStatistics)",
    "returns = np.array([t.pnl for t in trades])",
    "sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(252) if np.std(returns) > 0 else 0.0",
    "",
    "# Calculate min/max PnL",
    "pnls = [t.pnl for t in trades]",
    "max_pnl = max(pnls)",
    "min_pnl = min(pnls)",
    "",
    "# Calculate win/loss ratio",
    "win_loss_ratio = abs(stats.avg_winner / stats.avg_loser) if stats.avg_loser and stats.avg_loser != 0 else 0.0",
    "loss_rate = 1 - stats.win_rate",
    "",
    "print(\"=\"*60)",
    "print(\"TRADE STATISTICS\")",
    "print(\"=\"*60)",
    "print(f\"Total trades:        {stats.n_trades}\")",
    "print(f\"Winners:             {stats.n_winners} ({stats.win_rate:.1%})\")",
    "print(f\"Losers:              {stats.n_losers} ({loss_rate:.1%})\")",
    "print(f\"\")",
    "print(f\"Average PnL:         ${stats.avg_pnl:,.2f}\")",
    "print(f\"Total PnL:           ${stats.total_pnl:,.2f}\")",
    "print(f\"\")",
    "print(f\"Best trade:          ${max_pnl:,.2f}\")",
    "print(f\"Worst trade:         ${min_pnl:,.2f}\")",
    "print(f\"\")",
    "print(f\"Average winner:      ${stats.avg_winner if stats.avg_winner else 0:,.2f}\")",
    "print(f\"Average loser:       ${stats.avg_loser if stats.avg_loser else 0:,.2f}\")",
    "print(f\"Win/Loss ratio:      {win_loss_ratio:.2f}\")",
    "print(f\"\")",
    "print(f\"Sharpe Ratio:        {sharpe_ratio:.2f}\")",
    "print(f\"Profit Factor:       {stats.profit_factor if stats.profit_factor else 0:.2f}\")",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PnL distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PnL histogram\n",
    "pnls = [t.pnl for t in trades]\n",
    "axes[0].hist(pnls, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Break-even')\n",
    "axes[0].set_xlabel('PnL ($)', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Trade PnL Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative PnL\n",
    "sorted_trades = sorted(trades, key=lambda t: t.timestamp)\n",
    "cumulative_pnl = np.cumsum([t.pnl for t in sorted_trades])\n",
    "timestamps = [t.timestamp for t in sorted_trades]\n",
    "\n",
    "axes[1].plot(timestamps, cumulative_pnl, linewidth=2)\n",
    "axes[1].axhline(0, color='red', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('Date', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative PnL ($)', fontsize=12)\n",
    "axes[1].set_title('Cumulative PnL Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Final cumulative PnL: ${cumulative_pnl[-1]:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Validation with Deflated Sharpe Ratio (DSR)\n",
    "\n",
    "The **Deflated Sharpe Ratio** (Bailey & L\u00f3pez de Prado, 2014) corrects for multiple testing bias.\n",
    "\n",
    "When you test 100 strategies and pick the best one, you need to account for selection bias. DSR adjusts the Sharpe ratio to answer: **\"What's the probability this strategy is truly profitable, given I tested N strategies?\"**\n",
    "\n",
    "### Interpretation:\n",
    "- **DSR > 0.95**: Very confident the strategy is profitable\n",
    "- **DSR > 0.80**: Good confidence\n",
    "- **DSR > 0.50**: More likely profitable than not\n",
    "- **DSR < 0.50**: Likely just luck from multiple testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate returns for DSRreturns = np.array([t.pnl for t in trades]) / 100000  # Assume $100k capital per trade# DSR parametersn_trials = 100  # Assume we tested 100 strategiesvariance_trials = 0.15  # Variance of Sharpe ratios across trials (typical value)n_samples = len(returns)# Calculate momentsskewness = float(pd.Series(returns).skew())kurtosis = float(pd.Series(returns).kurtosis() + 3)  # Convert excess kurtosis to kurtosis# Calculate Sharpe Ratioreturns_for_sharpe = np.array([t.pnl for t in trades])sharpe_ratio = np.mean(returns_for_sharpe) / np.std(returns_for_sharpe) * np.sqrt(252) if np.std(returns_for_sharpe) > 0 else 0.0# Calculate DSRdsr_result = deflated_sharpe_ratio(    observed_sharpe=sharpe_ratio,    n_trials=n_trials,    variance_trials=variance_trials,    n_samples=n_samples,    skewness=skewness,    kurtosis=kurtosis,    return_components=True,    return_format='probability')print(\"=\"*60)print(\"DEFLATED SHARPE RATIO (DSR) ANALYSIS\")print(\"=\"*60)print(f\"Observed Sharpe Ratio:    {sharpe_ratio:.3f}\")print(f\"Number of trials tested:  {n_trials}\")print(f\"Number of samples:        {n_samples}\")print(f\"\")print(f\"Return distribution:\")print(f\"  Skewness:               {skewness:.3f}\")print(f\"  Kurtosis:               {kurtosis:.3f}\")print(f\"\")print(f\"Expected max SR (random): {dsr_result['expected_max_sharpe']:.3f}\")print(f\"Std dev of max SR:        {dsr_result['std_sharpe']:.3f}\")print(f\"\")print(f\"Z-score (standardized):   {dsr_result['dsr_zscore']:.3f}\")print(f\"\")print(f\"\u2b50 Deflated SR (DSR):     {dsr_result['dsr']:.3f}\")print(f\"\")print(\"=\"*60)# Interpretationif dsr_result['dsr'] > 0.95:    interpretation = \"\u2705 Very high confidence - Strategy likely profitable\"elif dsr_result['dsr'] > 0.80:    interpretation = \"\u2705 Good confidence - Strategy shows promise\"elif dsr_result['dsr'] > 0.50:    interpretation = \"\u26a0\ufe0f  Moderate confidence - More likely profitable than not\"else:    interpretation = \"\u274c Low confidence - Likely selection bias from multiple testing\"print(f\"\\nInterpretation: {interpretation}\")print(f\"\\nThis means: There's a {dsr_result['dsr']:.1%} probability that the true\")print(f\"Sharpe ratio is positive, accounting for testing {n_trials} strategies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SHAP Analysis of Worst Trades\n",
    "\n",
    "Now let's use **SHAP (SHapley Additive exPlanations)** to understand why our worst trades failed.\n",
    "\n",
    "SHAP values tell us:\n",
    "- Which features contributed most to each trade's outcome\n",
    "- Whether each feature pushed toward profit or loss\n",
    "- The magnitude of each feature's impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock model (in real usage, this would be your trained model)\n",
    "# For this demo, we just need it to exist for the API\n",
    "mock_model = lgb.LGBMClassifier(n_estimators=10, random_state=42, verbosity=-1)\n",
    "\n",
    "# Create TradeShapAnalyzer\n",
    "shap_analyzer = TradeShapAnalyzer(\n",
    "    model=mock_model,\n",
    "    features_df=features_df,\n",
    "    shap_values=shap_values\n",
    ")\n",
    "\n",
    "print(\"\u2705 TradeShapAnalyzer initialized\")\n",
    "print(f\"   Features: {len(feature_names)}\")\n",
    "print(f\"   Trades to analyze: {len(worst_trades)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the worst trade\n",
    "worst_trade = worst_trades[0]\n",
    "explanation = shap_analyzer.explain_trade(worst_trade)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"WORST TRADE EXPLANATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Symbol:          {worst_trade.symbol}\")\n",
    "print(f\"Timestamp:       {worst_trade.timestamp}\")\n",
    "print(f\"PnL:             ${worst_trade.pnl:,.2f}\")\n",
    "print(f\"Return:          {worst_trade.return_pct:.2f}%\")\n",
    "print(f\"Duration:        {worst_trade.duration_days:.1f} days\")\n",
    "print(f\"\")\n",
    "print(f\"Top 5 Feature Contributors:\")\n",
    "print(f\"{'-'*60}\")\n",
    "\n",
    "for feature, shap_val in explanation.top_features[:5]:\n",
    "    direction = \"\u2192 LOSS\" if shap_val > 0 else \"\u2192 PROFIT\"\n",
    "    print(f\"  {feature:20s}  {shap_val:+.3f}  {direction}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SHAP waterfall for worst trade\n",
    "top_features = explanation.top_features[:8]\n",
    "feature_labels = [f[0] for f in top_features]\n",
    "shap_vals = [f[1] for f in top_features]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create waterfall\n",
    "colors = ['red' if v > 0 else 'green' for v in shap_vals]\n",
    "y_pos = np.arange(len(feature_labels))\n",
    "\n",
    "ax.barh(y_pos, shap_vals, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(feature_labels)\n",
    "ax.axvline(0, color='black', linewidth=1)\n",
    "ax.set_xlabel('SHAP Value (contribution to loss)', fontsize=12)\n",
    "ax.set_title(f'SHAP Explanation: Worst Trade ({worst_trade.symbol})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='red', alpha=0.7, label='Contributed to loss'),\n",
    "    Patch(facecolor='green', alpha=0.7, label='Mitigated loss')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Pattern Clustering\n",
    "\n",
    "Instead of analyzing trades one-by-one, let's cluster them by **SHAP similarity** to find recurring error patterns.\n",
    "\n",
    "This reveals:\n",
    "- **How many distinct failure modes** exist\n",
    "- **Which features** define each pattern\n",
    "- **How many trades** fall into each pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster error patterns\n",
    "clustering_result = shap_analyzer.cluster_error_patterns(\n",
    "    trades=worst_trades,\n",
    "    n_clusters=3,\n",
    "    min_cluster_size=3\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ERROR PATTERN CLUSTERING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total worst trades analyzed: {len(worst_trades)}\")\n",
    "print(f\"Patterns discovered:         {len(clustering_result.patterns)}\")\n",
    "print(f\"\")\n",
    "\n",
    "for i, pattern in enumerate(clustering_result.patterns, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PATTERN {i}: {pattern.n_trades} trades\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Description: {pattern.description}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Top Features:\")\n",
    "    for feat_name, shap_mean, shap_std, pval, significant in pattern.top_features[:3]:\n",
    "        sig_marker = \"***\" if significant else \"\"\n",
    "        print(f\"  {feat_name:20s}  SHAP={shap_mean:+.3f} \u00b1 {shap_std:.3f}  {sig_marker}\")\n",
    "    print(f\"\")\n",
    "    print(f\"Separation score:     {pattern.separation_score:.2f}\")\n",
    "    print(f\"Distinctiveness:      {pattern.distinctiveness:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize pattern distribution\n",
    "pattern_sizes = [p.n_trades for p in clustering_result.patterns]\n",
    "pattern_labels = [f\"Pattern {i+1}\\n({p.n_trades} trades)\" \n",
    "                  for i, p in enumerate(clustering_result.patterns)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "colors = plt.cm.Set3(range(len(pattern_sizes)))\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    pattern_sizes,\n",
    "    labels=pattern_labels,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    startangle=90,\n",
    "    textprops={'fontsize': 11}\n",
    ")\n",
    "\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "ax.set_title('Distribution of Error Patterns', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hypothesis Generation & Actionable Insights\n",
    "\n",
    "For each error pattern, ml4t-diagnostic generates:\n",
    "1. **Hypothesis**: Why this pattern is causing losses\n",
    "2. **Actions**: Specific steps to fix it\n",
    "3. **Confidence**: How certain we are about the diagnosis\n",
    "\n",
    "This closes the **ML \u2192 Trading feedback loop**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hypotheses for each pattern\n",
    "print(\"=\"*70)\n",
    "print(\"ACTIONABLE HYPOTHESES & IMPROVEMENT RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, pattern in enumerate(clustering_result.patterns, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PATTERN {i}: {pattern.n_trades} trades (Confidence: {pattern.confidence:.0%})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcca Diagnosis:\")\n",
    "    print(f\"   {pattern.description}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udca1 Hypothesis:\")\n",
    "    print(f\"   {pattern.hypothesis}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udd27 Recommended Actions:\")\n",
    "    for j, action in enumerate(pattern.actions, 1):\n",
    "        print(f\"   {j}. {action}\")\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcc8 Expected Impact:\")\n",
    "    avg_loss = sum(t.pnl for t in worst_trades if hasattr(t, 'cluster_id') and \n",
    "                   t.cluster_id == pattern.cluster_id) / pattern.n_trades\n",
    "    potential_savings = abs(avg_loss) * pattern.n_trades\n",
    "    print(f\"   Avg loss per trade: ${avg_loss:,.2f}\")\n",
    "    print(f\"   Potential savings:  ${potential_savings:,.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Dashboard Exploration\n",
    "\n",
    "ml4t-diagnostic includes a **Streamlit dashboard** for interactive exploration.\n",
    "\n",
    "### Features:\n",
    "- **Tab 1**: DSR statistical validation\n",
    "- **Tab 2**: Interactive trade table with sorting/filtering\n",
    "- **Tab 3**: SHAP waterfall plots for any trade\n",
    "- **Tab 4**: Pattern cluster visualization\n",
    "\n",
    "### How to Launch:\n",
    "\n",
    "```bash\n",
    "# Option 1: With data from this notebook\n",
    "# (Save data first, then load in dashboard script)\n",
    "\n",
    "# Option 2: Standalone demo\n",
    "streamlit run examples/trade_shap_dashboard_demo.py\n",
    "```\n",
    "\n",
    "See `examples/trade_shap_dashboard_demo.py` for the demo script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dashboard is available\n",
    "try:\n",
    "    from ml4t.diagnostic.evaluation import run_diagnostics_dashboard\n",
    "    dashboard_available = True\n",
    "except ImportError:\n",
    "    dashboard_available = False\n",
    "\n",
    "if dashboard_available:\n",
    "    print(\"\u2705 Dashboard available!\")\n",
    "    print(\"\\nTo launch dashboard with this data:\")\n",
    "    print(\"\\n1. Run this in a Python script (not notebook):\")\n",
    "    print(\"   streamlit run examples/trade_shap_dashboard_demo.py\")\n",
    "    print(\"\\n2. Or programmatically:\")\n",
    "    print(\"   from ml4t.diagnostic.evaluation import run_diagnostics_dashboard\")\n",
    "    print(\"   run_diagnostics_dashboard(result=shap_result)\")\n",
    "else:\n",
    "    print(\"\u274c Dashboard not available\")\n",
    "    print(\"\\nInstall with: pip install ml4t-diagnostic[dashboard]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary & Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "\u2705 **Loaded backtest results** - 75 trades over 6 months  \n",
    "\u2705 **Analyzed trade statistics** - 40% win rate, Sharpe 0.8  \n",
    "\u2705 **Validated with DSR** - Accounted for multiple testing bias  \n",
    "\u2705 **Used SHAP to explain failures** - Identified feature contributions  \n",
    "\u2705 **Discovered 3 error patterns** - Clustered by SHAP similarity  \n",
    "\u2705 **Generated actionable hypotheses** - Specific improvement steps  \n",
    "\n",
    "### Key Insights from Our Analysis\n",
    "\n",
    "**Pattern 1** (High momentum + Low volatility \u2192 Losses):\n",
    "- **Issue**: Entering low-volatility momentum trends that reverse quickly\n",
    "- **Action**: Add volatility filter, use adaptive position sizing\n",
    "\n",
    "**Pattern 2** (Low liquidity + Wide spreads \u2192 Losses):\n",
    "- **Issue**: Poor execution quality in illiquid markets\n",
    "- **Action**: Add liquidity threshold, use limit orders\n",
    "\n",
    "**Pattern 3** (Regime changes + Correlation breaks \u2192 Losses):\n",
    "- **Issue**: Market regime shifts causing strategy failures\n",
    "- **Action**: Implement regime detection, add correlation filters\n",
    "\n",
    "### Implementation Roadmap\n",
    "\n",
    "**Phase 1 - Quick Wins (1-2 weeks)**:\n",
    "1. Add volatility filter (Pattern 1)\n",
    "2. Add liquidity threshold (Pattern 2)\n",
    "3. Retrain model with existing features\n",
    "\n",
    "**Phase 2 - Feature Engineering (2-4 weeks)**:\n",
    "1. Add regime detection features (Pattern 3)\n",
    "2. Add mean-reversion indicators (Patterns 1, 2)\n",
    "3. Add correlation metrics (Pattern 3)\n",
    "\n",
    "**Phase 3 - Validation (2-3 weeks)**:\n",
    "1. Out-of-sample testing\n",
    "2. Walk-forward validation\n",
    "3. Paper trading\n",
    "\n",
    "**Phase 4 - Production (ongoing)**:\n",
    "1. Deploy improved model\n",
    "2. Monitor performance\n",
    "3. Iterate with new SHAP diagnostics\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **Documentation**: `/docs/DASHBOARD.md`\n",
    "- **Examples**: `/examples/trade_shap_dashboard_demo.py`\n",
    "- **API Reference**: See module docstrings in `ml4t.diagnostic.evaluation`\n",
    "\n",
    "### References\n",
    "\n",
    "1. Bailey, D. H., & L\u00f3pez de Prado, M. (2014). \"The Deflated Sharpe Ratio\"\n",
    "2. Lundberg, S. M., & Lee, S. I. (2017). \"A Unified Approach to Interpreting Model Predictions\"\n",
    "3. L\u00f3pez de Prado, M. (2018). \"Advances in Financial Machine Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statisticsprint(\"=\"*70)print(\"FINAL SUMMARY\")print(\"=\"*70)print(f\"\\nBacktest Performance:\")print(f\"  Total trades:              {len(trades)}\")print(f\"  Win rate:                  {stats.win_rate:.1%}\")print(f\"  Sharpe Ratio:              {sharpe_ratio:.2f}\")print(f\"  Deflated SR:               {dsr_result['dsr']:.3f}\")print(f\"  Total PnL:                 ${stats.total_pnl:,.2f}\")print(f\"\\nDiagnostics:\")print(f\"  Worst trades analyzed:     {len(worst_trades)}\")print(f\"  Error patterns found:      {len(clustering_result.patterns)}\")print(f\"  Actionable hypotheses:     {len(clustering_result.patterns)}\")print(f\"\\nNext Steps:\")print(f\"  1. Implement recommended filters\")print(f\"  2. Engineer new features\")print(f\"  3. Retrain and validate\")print(f\"  4. Monitor in paper trading\")print(f\"\\n{'='*70}\")print(\"\\n\u2705 Analysis complete! Ready to improve your strategy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
