{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Signal Analysis with ML4T Diagnostic\n",
    "\n",
    "This notebook demonstrates how to analyze 50-200 trading signals simultaneously using the `MultiSignalAnalysis` module. Key features:\n",
    "\n",
    "1. **Batch Analysis**: Efficiently analyze many signals with parallel computation\n",
    "2. **Multiple Testing Corrections**: FDR (Benjamini-Hochberg) and FWER (Holm-Bonferroni)\n",
    "3. **Signal Selection**: Intelligent algorithms to identify best signals\n",
    "4. **Interactive Visualization**: Focus+Context dashboards for exploration\n",
    "\n",
    "## References\n",
    "- Benjamini & Hochberg (1995). \"Controlling the False Discovery Rate\"\n",
    "- Holm (1979). \"A Simple Sequentially Rejective Multiple Test Procedure\"\n",
    "- Lopez de Prado (2018). \"Advances in Financial Machine Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from ml4t.diagnostic.config import MultiSignalAnalysisConfig\n",
    "\n",
    "# ML4T Diagnostic imports\n",
    "from ml4t.diagnostic.evaluation import (\n",
    "    MultiSignalAnalysis,\n",
    "    SignalSelector,\n",
    ")\n",
    "from ml4t.diagnostic.visualization import (\n",
    "    MultiSignalDashboard,\n",
    "    plot_ic_ridge,\n",
    "    plot_pareto_frontier,\n",
    "    plot_signal_correlation_heatmap,\n",
    "    plot_signal_ranking_bar,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "We'll create a synthetic dataset with:\n",
    "- 30 assets over 2 years (500 trading days)\n",
    "- 20 signals with varying predictive power\n",
    "- Some signals correlated with each other (cluster structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trading_dates(start: str, n_days: int) -> list[datetime]:\n",
    "    \"\"\"Generate trading dates (skip weekends).\"\"\"\n",
    "    dates = []\n",
    "    current = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    while len(dates) < n_days:\n",
    "        if current.weekday() < 5:  # Mon-Fri\n",
    "            dates.append(current)\n",
    "        current += timedelta(days=1)\n",
    "    return dates\n",
    "\n",
    "\n",
    "def generate_price_data(\n",
    "    dates: list[datetime],\n",
    "    assets: list[str],\n",
    "    seed: int = 42,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Generate synthetic price data with random walk.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    records = []\n",
    "    for asset in assets:\n",
    "        price = 100.0 * (1 + np.random.randn() * 0.3)  # Random starting price\n",
    "        for date in dates:\n",
    "            price *= 1 + np.random.randn() * 0.02  # Daily volatility ~2%\n",
    "            records.append({\"date\": date, \"asset\": asset, \"price\": max(price, 1.0)})\n",
    "    return pl.DataFrame(records)\n",
    "\n",
    "\n",
    "def generate_signals(\n",
    "    dates: list[datetime],\n",
    "    assets: list[str],\n",
    "    n_signals: int,\n",
    "    n_clusters: int = 4,\n",
    "    seed: int = 42,\n",
    ") -> dict[str, pl.DataFrame]:\n",
    "    \"\"\"Generate signals with cluster structure and varying IC.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    signals = {}\n",
    "\n",
    "    # Assign signals to clusters\n",
    "    cluster_assignments = np.arange(n_signals) % n_clusters\n",
    "\n",
    "    # Generate cluster base signals\n",
    "    cluster_bases = {}\n",
    "    for c in range(n_clusters):\n",
    "        cluster_bases[c] = np.random.randn(len(dates), len(assets))\n",
    "\n",
    "    for sig_idx in range(n_signals):\n",
    "        cluster = cluster_assignments[sig_idx]\n",
    "\n",
    "        # Signal is cluster base + noise\n",
    "        noise_level = 0.3 + np.random.rand() * 0.5\n",
    "        factor_values = (\n",
    "            cluster_bases[cluster] * (1 - noise_level)\n",
    "            + np.random.randn(len(dates), len(assets)) * noise_level\n",
    "        )\n",
    "\n",
    "        # Create DataFrame\n",
    "        records = []\n",
    "        for i, date in enumerate(dates):\n",
    "            for j, asset in enumerate(assets):\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"date\": date,\n",
    "                        \"asset\": asset,\n",
    "                        \"factor\": factor_values[i, j],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Name with cluster prefix for interpretability\n",
    "        signal_name = f\"cluster{cluster}_signal_{sig_idx:02d}\"\n",
    "        signals[signal_name] = pl.DataFrame(records)\n",
    "\n",
    "    return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "N_DAYS = 500\n",
    "N_ASSETS = 30\n",
    "N_SIGNALS = 20\n",
    "N_CLUSTERS = 4\n",
    "\n",
    "# Generate data\n",
    "dates = generate_trading_dates(\"2022-01-03\", N_DAYS)\n",
    "assets = [f\"STOCK_{i:02d}\" for i in range(N_ASSETS)]\n",
    "\n",
    "prices = generate_price_data(dates, assets)\n",
    "signals = generate_signals(dates, assets, N_SIGNALS, N_CLUSTERS)\n",
    "\n",
    "print(f\"Price data: {prices.shape}\")\n",
    "print(f\"Number of signals: {len(signals)}\")\n",
    "print(f\"Signal names: {list(signals.keys())[:5]} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Multi-Signal Analysis\n",
    "\n",
    "The `MultiSignalAnalysis` class provides batch analysis with:\n",
    "- Parallel computation via joblib\n",
    "- Smart caching with Polars fingerprinting\n",
    "- Multiple testing corrections (FDR, FWER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analyzer with default configuration\n",
    "analyzer = MultiSignalAnalysis(\n",
    "    signals=signals,\n",
    "    prices=prices,\n",
    ")\n",
    "\n",
    "# Compute summary metrics for all signals\n",
    "summary = analyzer.compute_summary(progress=True)\n",
    "\n",
    "print(f\"\\nTotal signals: {summary.n_signals}\")\n",
    "print(f\"FDR significant (alpha=0.05): {summary.n_fdr_significant}\")\n",
    "print(f\"FWER significant (alpha=0.05): {summary.n_fwer_significant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View summary as DataFrame\n",
    "df = summary.get_dataframe()\n",
    "print(\"Summary DataFrame columns:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Show top signals by IC IR\n",
    "print(\"\\nTop 5 signals by IC IR:\")\n",
    "df.sort(\"ic_ir\", descending=True).head(5).select(\n",
    "    [\"signal_name\", \"ic_mean\", \"ic_ir\", \"ic_t_stat\", \"fdr_significant\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Configuration\n",
    "\n",
    "Configure the analysis for your specific needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml4t.diagnostic.config import SignalAnalysisConfig\n",
    "\n",
    "# Strict configuration for production\n",
    "strict_config = MultiSignalAnalysisConfig(\n",
    "    # Tighter significance thresholds\n",
    "    fdr_alpha=0.01,\n",
    "    fwer_alpha=0.01,\n",
    "    # Minimum IC threshold\n",
    "    min_ic_threshold=0.02,\n",
    "    # Performance tuning\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    cache_enabled=True,\n",
    "    # Analysis periods\n",
    "    signal_config=SignalAnalysisConfig(\n",
    "        periods=(1, 5, 10),  # Forward return periods\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Run with strict config\n",
    "strict_analyzer = MultiSignalAnalysis(signals, prices, config=strict_config)\n",
    "strict_summary = strict_analyzer.compute_summary(progress=True)\n",
    "\n",
    "print(\"\\nWith strict thresholds (alpha=0.01):\")\n",
    "print(f\"FDR significant: {strict_summary.n_fdr_significant}\")\n",
    "print(f\"FWER significant: {strict_summary.n_fwer_significant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Signal Selection Algorithms\n",
    "\n",
    "Use `SignalSelector` to identify the most promising signals from a large universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary DataFrame for selection\n",
    "df = summary.get_dataframe()\n",
    "\n",
    "# Method 1: Top N by IC IR\n",
    "top_by_ir = SignalSelector.select_top_n(\n",
    "    summary_df=df,\n",
    "    n=5,\n",
    "    metric=\"ic_ir\",\n",
    ")\n",
    "print(\"Top 5 by IC IR:\")\n",
    "print(top_by_ir)\n",
    "\n",
    "# Method 2: Top N with low turnover\n",
    "low_turnover = SignalSelector.select_top_n(\n",
    "    summary_df=df,\n",
    "    n=5,\n",
    "    metric=\"turnover_mean\",\n",
    "    ascending=True,  # Lowest turnover\n",
    ")\n",
    "print(\"\\nTop 5 lowest turnover:\")\n",
    "print(low_turnover)\n",
    "\n",
    "# Method 3: Only FDR-significant signals\n",
    "significant_only = SignalSelector.select_top_n(\n",
    "    summary_df=df,\n",
    "    n=5,\n",
    "    metric=\"ic_ir\",\n",
    "    filter_significant=True,\n",
    ")\n",
    "print(\"\\nTop 5 FDR-significant:\")\n",
    "print(significant_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Select uncorrelated signals\n",
    "# First get correlation matrix\n",
    "correlation_matrix = summary.get_correlation_matrix()\n",
    "\n",
    "if correlation_matrix is not None:\n",
    "    uncorrelated = SignalSelector.select_uncorrelated(\n",
    "        summary_df=df,\n",
    "        correlation_df=correlation_matrix,\n",
    "        n=5,\n",
    "        max_correlation=0.5,  # Exclude pairs with |corr| > 0.5\n",
    "    )\n",
    "    print(\"Top 5 uncorrelated signals:\")\n",
    "    print(uncorrelated)\n",
    "else:\n",
    "    print(\"No correlation matrix available - run with compute_correlation=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 5: Pareto frontier (IC IR vs Turnover)\n",
    "pareto_signals = SignalSelector.select_pareto_frontier(\n",
    "    summary_df=df,\n",
    "    x_metric=\"turnover_mean\",  # Minimize\n",
    "    y_metric=\"ic_ir\",  # Maximize\n",
    ")\n",
    "print(\"Pareto-optimal signals (IC IR vs Turnover):\")\n",
    "print(pareto_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 6: Cluster-based selection (one representative per cluster)\n",
    "if correlation_matrix is not None:\n",
    "    cluster_reps = SignalSelector.select_by_cluster(\n",
    "        correlation_df=correlation_matrix,\n",
    "        summary_df=df,\n",
    "        n_clusters=4,\n",
    "        signals_per_cluster=1,\n",
    "    )\n",
    "    print(\"Cluster representatives:\")\n",
    "    print(cluster_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "\n",
    "Create interactive visualizations using the Focus+Context pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC Ridge Plot - shows IC distribution per signal\n",
    "fig = plot_ic_ridge(\n",
    "    summary,\n",
    "    max_signals=15,\n",
    "    sort_by=\"ic_mean\",\n",
    "    show_significance=True,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal Ranking Bar Chart\n",
    "fig = plot_signal_ranking_bar(\n",
    "    summary,\n",
    "    metric=\"ic_ir\",\n",
    "    max_signals=15,\n",
    "    show_significance=True,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal Correlation Heatmap (with hierarchical clustering)\n",
    "fig = plot_signal_correlation_heatmap(\n",
    "    summary,\n",
    "    max_signals=20,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto Frontier (IC IR vs Turnover)\n",
    "fig = plot_pareto_frontier(\n",
    "    summary,\n",
    "    x_metric=\"turnover_mean\",\n",
    "    y_metric=\"ic_ir\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Signal Dashboard\n",
    "\n",
    "The `MultiSignalDashboard` combines all visualizations into an interactive HTML dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-tab dashboard\n",
    "dashboard = MultiSignalDashboard(summary)\n",
    "\n",
    "# Save as HTML file\n",
    "dashboard.save_html(\"multi_signal_dashboard.html\")\n",
    "print(\"Dashboard saved to multi_signal_dashboard.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or get as HTML string for embedding\n",
    "html_content = dashboard.to_html()\n",
    "print(f\"Dashboard HTML size: {len(html_content):,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Signal Comparison\n",
    "\n",
    "Compare selected signals in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top signals for comparison\n",
    "top_signals = SignalSelector.select_top_n(df, n=3, metric=\"ic_ir\")\n",
    "\n",
    "# Run comparison\n",
    "comparison = analyzer.compare(signal_names=top_signals)\n",
    "\n",
    "print(f\"Comparing: {comparison.signal_names}\")\n",
    "print(\"\\nComparison metrics:\")\n",
    "print(comparison.metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison as HTML\n",
    "comparison.save_html(\"signal_comparison.html\")\n",
    "print(\"Comparison saved to signal_comparison.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Working with Large Signal Sets\n",
    "\n",
    "Tips for handling 100+ signals efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for large signal sets\n",
    "large_config = MultiSignalAnalysisConfig(\n",
    "    # Enable caching - huge speedup for re-analysis\n",
    "    cache_enabled=True,\n",
    "    cache_max_items=500,\n",
    "    # Use all cores\n",
    "    n_jobs=-1,\n",
    "    # Faster single-period analysis for initial screening\n",
    "    signal_config=SignalAnalysisConfig(\n",
    "        periods=(5,),  # Just 5-day forward returns\n",
    "    ),\n",
    "    # Display limits\n",
    "    max_display_signals=50,\n",
    ")\n",
    "\n",
    "print(\"Configuration for large signal sets:\")\n",
    "print(f\"  Cache enabled: {large_config.cache_enabled}\")\n",
    "print(f\"  N jobs: {large_config.n_jobs}\")\n",
    "print(f\"  Periods: {large_config.signal_config.periods}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Key takeaways for multi-signal analysis:\n",
    "\n",
    "1. **Always apply multiple testing corrections** - FDR or FWER to avoid false discoveries\n",
    "2. **Use signal selection algorithms** - Don't just take top N by raw IC\n",
    "3. **Consider correlations** - Uncorrelated signals provide more diversification\n",
    "4. **Trade off IC vs Turnover** - Pareto frontier shows efficient signals\n",
    "5. **Enable caching** - Speeds up re-analysis significantly\n",
    "6. **Use dashboards** - Interactive exploration is key for 50-200 signals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
