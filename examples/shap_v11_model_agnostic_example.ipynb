{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP v1.1: Model-Agnostic Feature Importance\n",
    "\n",
    "**NEW in ml4t-diagnostic v1.1**: SHAP importance now works with **ANY sklearn-compatible model**!\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. **TreeExplainer**: Fast, exact computation for tree models (LightGBM, XGBoost)\n",
    "2. **LinearExplainer**: Fast, exact computation for linear models (LogisticRegression)\n",
    "3. **KernelExplainer**: Model-agnostic fallback (SVM, KNN, ANY model)\n",
    "4. **Auto-Selection**: Automatic explainer selection based on model type\n",
    "5. **Performance Comparison**: Speed vs quality trade-offs\n",
    "6. **Best Practices**: Tips for using each explainer effectively\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "# Standard ML support (Tree, Linear, Kernel explainers)\n",
    "pip install ml4t-diagnostic[ml]\n",
    "\n",
    "# Neural network support (adds Deep explainer)\n",
    "pip install ml4t-diagnostic[deep]\n",
    "\n",
    "# GPU acceleration (10-50x speedup for large datasets)\n",
    "pip install ml4t-diagnostic[gpu]\n",
    "\n",
    "# Everything (all explainers + GPU)\n",
    "pip install ml4t-diagnostic[all-ml]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "\n",
    "# Models\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import polars as pl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from ml4t.diagnostic.evaluation import compute_shap_importance\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Trading Data\n",
    "\n",
    "Create a realistic quantitative trading dataset with:\n",
    "- **Features**: momentum, volatility, volume, spread, etc.\n",
    "- **Target**: Binary classification (trade success/failure)\n",
    "- **Signal**: momentum + volatility interaction (typical quant pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic trading data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 10\n",
    "\n",
    "# Feature names\n",
    "feature_names = [\n",
    "    \"momentum_5d\",\n",
    "    \"momentum_20d\",\n",
    "    \"volatility_5d\",\n",
    "    \"volatility_20d\",\n",
    "    \"volume_ratio\",\n",
    "    \"spread\",\n",
    "    \"rsi\",\n",
    "    \"macd\",\n",
    "    \"atr\",\n",
    "    \"beta\",\n",
    "]\n",
    "\n",
    "# Generate features\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Create target with momentum + volatility interaction (realistic quant signal)\n",
    "signal = (\n",
    "    0.5 * X[:, 0]  # momentum_5d (strong)\n",
    "    + 0.3 * X[:, 2]  # volatility_5d (medium)\n",
    "    + 0.2 * X[:, 0] * X[:, 2]  # interaction\n",
    "    + 0.1 * np.random.randn(n_samples)  # noise\n",
    ")\n",
    "y = (signal > 0).astype(int)\n",
    "\n",
    "# Convert to Polars DataFrame\n",
    "X_df = pl.DataFrame(X, schema=feature_names)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_df.to_numpy(), y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Class balance: {np.mean(y_train):.2%} positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TreeExplainer: Fast, Exact for Tree Models\n",
    "\n",
    "**Best for**: LightGBM, XGBoost, RandomForest  \n",
    "**Speed**: <10ms per sample  \n",
    "**Quality**: Exact SHAP values  \n",
    "**Use when**: You have tree-based models (most quant ML workflows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM model\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42, verbose=-1\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Compute SHAP importance (auto-selects TreeExplainer)\n",
    "start_time = time.time()\n",
    "result_tree = compute_shap_importance(model=lgb_model, X=X_test, feature_names=feature_names)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Display results\n",
    "print(f\"Explainer used: {result_tree['explainer_type']}\")\n",
    "print(f\"Computation time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Time per sample: {elapsed_time / result_tree['n_samples'] * 1000:.2f}ms\\n\")\n",
    "\n",
    "print(\"Top 5 features:\")\n",
    "for feat, imp in zip(result_tree[\"feature_names\"][:5], result_tree[\"importances\"][:5]):\n",
    "    print(f\"  {feat:20s}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LinearExplainer: Fast, Exact for Linear Models\n",
    "\n",
    "**Best for**: LogisticRegression, Ridge, Lasso, LinearSVM  \n",
    "**Speed**: <100ms per sample  \n",
    "**Quality**: Exact SHAP values  \n",
    "**Use when**: You have linear models (e.g., factor models, simple baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression(penalty=\"l2\", C=1.0, max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Compute SHAP importance (auto-selects LinearExplainer)\n",
    "start_time = time.time()\n",
    "result_linear = compute_shap_importance(model=lr_model, X=X_test, feature_names=feature_names)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Display results\n",
    "print(f\"Explainer used: {result_linear['explainer_type']}\")\n",
    "print(f\"Computation time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Time per sample: {elapsed_time / result_linear['n_samples'] * 1000:.2f}ms\\n\")\n",
    "\n",
    "print(\"Top 5 features:\")\n",
    "for feat, imp in zip(result_linear[\"feature_names\"][:5], result_linear[\"importances\"][:5]):\n",
    "    print(f\"  {feat:20s}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. KernelExplainer: Model-Agnostic Fallback\n",
    "\n",
    "**Best for**: SVM, KNN, ANY sklearn-compatible model  \n",
    "**Speed**: 100-5000ms per sample (SLOW!)  \n",
    "**Quality**: Approximate SHAP values  \n",
    "**Use when**: No specialized explainer available (universal fallback)\n",
    "\n",
    "‚ö†Ô∏è **Performance tip**: Use `max_samples` parameter to limit computation time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model (no specialized explainer available)\n",
    "svm_model = SVC(\n",
    "    kernel=\"rbf\",\n",
    "    C=1.0,\n",
    "    probability=True,  # Required for SHAP\n",
    "    random_state=42,\n",
    ")\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Compute SHAP importance (auto-selects KernelExplainer)\n",
    "# Use max_samples for speed (KernelExplainer is slow!)\n",
    "start_time = time.time()\n",
    "result_kernel = compute_shap_importance(\n",
    "    model=svm_model,\n",
    "    X=X_test,\n",
    "    feature_names=feature_names,\n",
    "    max_samples=50,  # Limit to 50 samples for demo (faster)\n",
    "    performance_warning=True,  # Warn if computation will be slow\n",
    "    show_progress=False,  # Set to True to see progress bar\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# Display results\n",
    "print(f\"Explainer used: {result_kernel['explainer_type']}\")\n",
    "print(f\"Computation time: {elapsed_time:.2f} seconds\")\n",
    "print(f\"Time per sample: {elapsed_time / result_kernel['n_samples'] * 1000:.2f}ms\\n\")\n",
    "\n",
    "print(\"Top 5 features:\")\n",
    "for feat, imp in zip(result_kernel[\"feature_names\"][:5], result_kernel[\"importances\"][:5]):\n",
    "    print(f\"  {feat:20s}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Note: KernelExplainer is MUCH slower than Tree/Linear explainers!\")\n",
    "print(\"   Always use max_samples parameter to limit computation time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Auto-Selection Behavior\n",
    "\n",
    "When `explainer_type='auto'` (default), the function tries explainers in order:\n",
    "\n",
    "1. **TreeExplainer**: Check for tree-like attributes (tree_, estimators_, booster_)\n",
    "2. **LinearExplainer**: Check for linear attributes (coef_, intercept_)\n",
    "3. **KernelExplainer**: Universal fallback (works for ANY model)\n",
    "\n",
    "You can override with explicit `explainer_type` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate auto-selection\n",
    "models = [(\"LightGBM\", lgb_model), (\"LogisticRegression\", lr_model), (\"SVM\", svm_model)]\n",
    "\n",
    "print(\"Auto-selection results:\\n\")\n",
    "for name, model in models:\n",
    "    result = compute_shap_importance(\n",
    "        model=model,\n",
    "        X=X_test[:10],  # Small subset for speed\n",
    "        feature_names=feature_names,\n",
    "        performance_warning=False,\n",
    "    )\n",
    "    print(f\"{name:25s} ‚Üí {result['explainer_type']:10s} explainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explicit Explainer Selection\n",
    "\n",
    "Force a specific explainer (useful for comparison or debugging):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare TreeExplainer vs KernelExplainer on same model\n",
    "print(\"Comparing explainers on LightGBM model:\\n\")\n",
    "\n",
    "# Default: TreeExplainer (fast, exact)\n",
    "start_time = time.time()\n",
    "result_tree = compute_shap_importance(\n",
    "    model=lgb_model, X=X_test, feature_names=feature_names, explainer_type=\"tree\"\n",
    ")\n",
    "time_tree = time.time() - start_time\n",
    "\n",
    "# Force: KernelExplainer (slow, approximate)\n",
    "start_time = time.time()\n",
    "result_kernel_lgb = compute_shap_importance(\n",
    "    model=lgb_model,\n",
    "    X=X_test,\n",
    "    feature_names=feature_names,\n",
    "    explainer_type=\"kernel\",\n",
    "    max_samples=50,  # Limit for speed\n",
    "    performance_warning=False,\n",
    ")\n",
    "time_kernel = time.time() - start_time\n",
    "\n",
    "# Compare\n",
    "print(\n",
    "    f\"TreeExplainer:   {time_tree:.2f}s ({time_tree / result_tree['n_samples'] * 1000:.2f}ms/sample)\"\n",
    ")\n",
    "print(\n",
    "    f\"KernelExplainer: {time_kernel:.2f}s ({time_kernel / result_kernel_lgb['n_samples'] * 1000:.2f}ms/sample)\"\n",
    ")\n",
    "print(f\"\\nSpeedup: {time_kernel / time_tree:.1f}x faster with TreeExplainer!\")\n",
    "\n",
    "print(\"\\nüí° Tip: Always use specialized explainers (Tree, Linear) when available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison Visualization\n",
    "\n",
    "Visualize speed vs quality trade-offs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect performance data\n",
    "explainers = [\"Tree\", \"Linear\", \"Kernel\"]\n",
    "times = [\n",
    "    time_tree / result_tree[\"n_samples\"] * 1000,\n",
    "    elapsed_time / result_linear[\"n_samples\"] * 1000,  # From earlier\n",
    "    time_kernel / result_kernel_lgb[\"n_samples\"] * 1000,\n",
    "]\n",
    "quality = [\"Exact\", \"Exact\", \"Approx\"]\n",
    "\n",
    "# Create bar chart\n",
    "fig = go.Figure(\n",
    "    [\n",
    "        go.Bar(\n",
    "            x=explainers,\n",
    "            y=times,\n",
    "            text=[f\"{t:.1f}ms\" for t in times],\n",
    "            textposition=\"auto\",\n",
    "            marker_color=[\"green\", \"blue\", \"red\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"SHAP Explainer Performance Comparison\",\n",
    "    xaxis_title=\"Explainer Type\",\n",
    "    yaxis_title=\"Time per Sample (ms, log scale)\",\n",
    "    yaxis_type=\"log\",\n",
    "    showlegend=False,\n",
    "    height=400,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüìä Key Takeaways:\")\n",
    "print(\"  ‚Ä¢ TreeExplainer: Fastest, exact (use for tree models)\")\n",
    "print(\"  ‚Ä¢ LinearExplainer: Fast, exact (use for linear models)\")\n",
    "print(\"  ‚Ä¢ KernelExplainer: Slowest, approximate (universal fallback)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Comparison\n",
    "\n",
    "Compare SHAP importance across different models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "import polars as pl\n",
    "\n",
    "comparison = pl.DataFrame(\n",
    "    {\n",
    "        \"Feature\": feature_names,\n",
    "        \"Tree (LightGBM)\": [\n",
    "            result_tree[\"importances\"][result_tree[\"feature_names\"].index(f)] for f in feature_names\n",
    "        ],\n",
    "        \"Linear (LogReg)\": [\n",
    "            result_linear[\"importances\"][result_linear[\"feature_names\"].index(f)]\n",
    "            for f in feature_names\n",
    "        ],\n",
    "        \"Kernel (SVM)\": [\n",
    "            result_kernel[\"importances\"][result_kernel[\"feature_names\"].index(f)]\n",
    "            for f in feature_names\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort by tree importance\n",
    "comparison = comparison.sort(\"Tree (LightGBM)\", descending=True)\n",
    "\n",
    "print(\"\\nFeature Importance Comparison:\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each model\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        name=\"Tree (LightGBM)\",\n",
    "        x=comparison[\"Feature\"].to_list(),\n",
    "        y=comparison[\"Tree (LightGBM)\"].to_list(),\n",
    "        marker_color=\"green\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        name=\"Linear (LogReg)\",\n",
    "        x=comparison[\"Feature\"].to_list(),\n",
    "        y=comparison[\"Linear (LogReg)\"].to_list(),\n",
    "        marker_color=\"blue\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        name=\"Kernel (SVM)\",\n",
    "        x=comparison[\"Feature\"].to_list(),\n",
    "        y=comparison[\"Kernel (SVM)\"].to_list(),\n",
    "        marker_color=\"red\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"SHAP Feature Importance: Model Comparison\",\n",
    "    xaxis_title=\"Feature\",\n",
    "    yaxis_title=\"Mean |SHAP value|\",\n",
    "    barmode=\"group\",\n",
    "    height=500,\n",
    "    xaxis_tickangle=-45,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nüìä Interpretation:\")\n",
    "print(\"  ‚Ä¢ momentum_5d: Top feature across all models (matches ground truth)\")\n",
    "print(\"  ‚Ä¢ volatility_5d: Important (momentum-volatility interaction)\")\n",
    "print(\"  ‚Ä¢ Different models capture different aspects of the signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices\n",
    "\n",
    "### TreeExplainer\n",
    "‚úÖ **Use for**: LightGBM, XGBoost, RandomForest  \n",
    "‚úÖ **Performance**: Fast (<10ms/sample)  \n",
    "‚úÖ **Quality**: Exact SHAP values  \n",
    "‚úÖ **Tip**: Default choice for tree models\n",
    "\n",
    "### LinearExplainer\n",
    "‚úÖ **Use for**: LogisticRegression, Ridge, Lasso  \n",
    "‚úÖ **Performance**: Fast (<100ms/sample)  \n",
    "‚úÖ **Quality**: Exact SHAP values  \n",
    "‚úÖ **Tip**: Great for factor models and baselines\n",
    "\n",
    "### KernelExplainer\n",
    "‚ö†Ô∏è **Use for**: SVM, KNN, ANY model (universal fallback)  \n",
    "‚ö†Ô∏è **Performance**: SLOW (100-5000ms/sample)  \n",
    "‚ö†Ô∏è **Quality**: Approximate SHAP values  \n",
    "‚ö†Ô∏è **Tip**: Always use `max_samples` parameter!\n",
    "\n",
    "```python\n",
    "# Good: Limit samples for speed\n",
    "result = compute_shap_importance(\n",
    "    model, X, \n",
    "    max_samples=100,  # Much faster\n",
    "    show_progress=True  # Show progress\n",
    ")\n",
    "\n",
    "# Bad: Full dataset (can take hours!)\n",
    "result = compute_shap_importance(model, X_large)  # Slow!\n",
    "```\n",
    "\n",
    "### GPU Acceleration\n",
    "üöÄ **Use for**: Large datasets (>10K samples)  \n",
    "üöÄ **Speedup**: 10-50x faster  \n",
    "üöÄ **Requires**: `pip install ml4t-diagnostic[gpu]`\n",
    "\n",
    "```python\n",
    "result = compute_shap_importance(\n",
    "    model, X_large,\n",
    "    use_gpu=True  # or 'auto' for automatic detection\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### v1.1 Key Features\n",
    "- ‚úÖ **Multi-Explainer Support**: Tree, Linear, Kernel, Deep\n",
    "- ‚úÖ **Universal Compatibility**: Works with ANY sklearn model\n",
    "- ‚úÖ **Smart Auto-Selection**: Automatically picks best explainer\n",
    "- ‚úÖ **100% Backward Compatible**: All v1.0 code works unchanged\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "```python\n",
    "from ml4t.diagnostic.evaluation import compute_shap_importance\n",
    "\n",
    "# Auto-selection (recommended)\n",
    "result = compute_shap_importance(model, X)\n",
    "print(f\"Used: {result['explainer_type']}\")  # 'tree', 'linear', 'kernel', 'deep'\n",
    "\n",
    "# Explicit selection\n",
    "result = compute_shap_importance(model, X, explainer_type='kernel')\n",
    "\n",
    "# Performance optimization\n",
    "result = compute_shap_importance(\n",
    "    model, X,\n",
    "    max_samples=100,  # Limit samples (for KernelExplainer)\n",
    "    use_gpu=True,  # Enable GPU (if available)\n",
    "    show_progress=True  # Show progress bar\n",
    ")\n",
    "```\n",
    "\n",
    "### When to Use Each Explainer\n",
    "\n",
    "| Model Type | Recommended Explainer | Speed | Quality |\n",
    "|------------|----------------------|-------|--------|\n",
    "| LightGBM, XGBoost | TreeExplainer | ‚úÖ Fast | ‚úÖ Exact |\n",
    "| LogisticRegression, Ridge | LinearExplainer | ‚úÖ Fast | ‚úÖ Exact |\n",
    "| TensorFlow, PyTorch | DeepExplainer | ‚ö†Ô∏è Medium | ‚ö†Ô∏è Approx |\n",
    "| SVM, KNN, Other | KernelExplainer | ‚ùå Slow | ‚ö†Ô∏è Approx |\n",
    "\n",
    "### Additional Resources\n",
    "- **Documentation**: See `compute_shap_importance` docstring\n",
    "- **Migration Guide**: `docs/MIGRATION.md`\n",
    "- **README**: `README.md` (v1.1 section)\n",
    "\n",
    "### Feedback\n",
    "Questions or issues? File an issue on GitHub!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
